{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* **In this Notebook, we are trying to approach the task of semantic segmentation, classifying each pixel in an image from a set of classes that are previously defined.**\n\n* **Our main goal in this task is to take an image of size (Width x Height x 3) and generate a (Width x Height) matrix containing the predicted class corresponding to each pixel in the image.**","metadata":{}},{"cell_type":"markdown","source":"As the first step in any computer vision task, we are going to prepare the dataset. \n\nIn this notebook, we will use [this dataset](https://www.kaggle.com/bulentsiyah/semantic-drone-dataset)","metadata":{}},{"cell_type":"markdown","source":"So, Let's first import the dependencies.","metadata":{}},{"cell_type":"code","source":"# Importing all the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom keras.utils import Sequence, to_categorical, plot_model\nfrom keras.layers import Conv2D, Dropout, MaxPooling2D, UpSampling2D, concatenate, Input\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:23:57.459472Z","iopub.execute_input":"2021-11-04T07:23:57.459801Z","iopub.status.idle":"2021-11-04T07:23:57.466689Z","shell.execute_reply.started":"2021-11-04T07:23:57.459725Z","shell.execute_reply":"2021-11-04T07:23:57.465900Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Now, Let's visualize the images in the dataset","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/semantic-drone-dataset/dataset/semantic_drone_dataset/'\nimg = cv2.imread(path + 'original_images/001.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nmask = cv2.imread(path + 'label_images_semantic/001.png', cv2.IMREAD_GRAYSCALE)\n#mask = mask.cvtColor(img, cv2.COLOR_BGR2RGB)\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(img)\naxs[1].imshow(mask)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:23:57.474107Z","iopub.execute_input":"2021-11-04T07:23:57.474325Z","iopub.status.idle":"2021-11-04T07:23:59.882821Z","shell.execute_reply.started":"2021-11-04T07:23:57.474283Z","shell.execute_reply":"2021-11-04T07:23:59.882127Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print('Image Dimensions are: ', img.shape)\nprint('Label Dimensions are: ', mask.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:23:59.884372Z","iopub.execute_input":"2021-11-04T07:23:59.884861Z","iopub.status.idle":"2021-11-04T07:23:59.891419Z","shell.execute_reply.started":"2021-11-04T07:23:59.884804Z","shell.execute_reply":"2021-11-04T07:23:59.890692Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"**DATA Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"Let's Prepare our dataset for the training","metadata":{}},{"cell_type":"code","source":"# Prepare the Images\nX = []\nfor filename in sorted(os.listdir(path + 'original_images/')):\n    a = cv2.imread(path + 'original_images/' + filename)\n    a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n    a = cv2.resize(a, (256, 256))\n    a = a / 255\n    X.append(a)\n    \nX = np.array(X)\n\n# Prepare the Labels\nY = []\nfor filename in sorted(os.listdir(path + 'label_images_semantic/')):\n    a = cv2.imread(path + 'label_images_semantic/' + filename, cv2.IMREAD_GRAYSCALE)\n    a = cv2.resize(a, (256, 256))\n    Y.append(a)\n    \nY = np.array(Y)\nYc = to_categorical(Y)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:23:59.893188Z","iopub.execute_input":"2021-11-04T07:23:59.893840Z","iopub.status.idle":"2021-11-04T07:28:20.012407Z","shell.execute_reply.started":"2021-11-04T07:23:59.893794Z","shell.execute_reply":"2021-11-04T07:28:20.007758Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(Yc.shape)\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(X[1])\naxs[1].imshow(Y[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:20.020605Z","iopub.execute_input":"2021-11-04T07:28:20.021444Z","iopub.status.idle":"2021-11-04T07:28:20.584906Z","shell.execute_reply.started":"2021-11-04T07:28:20.021381Z","shell.execute_reply":"2021-11-04T07:28:20.584156Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Now, Let's Split our data into training and validation.\n\nI will keep the last image as a test image to test the Model","metadata":{}},{"cell_type":"code","source":"test_image1 = X[-1]\ntest_label1 = Yc[-1]\ntest_image2 = X[-2]\ntest_label2 = Yc[-2]\nx_train, x_val, y_train, y_val = train_test_split(X[0:-2], Yc[0:-2], test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:20.587579Z","iopub.execute_input":"2021-11-04T07:28:20.587993Z","iopub.status.idle":"2021-11-04T07:28:21.888343Z","shell.execute_reply.started":"2021-11-04T07:28:20.587946Z","shell.execute_reply":"2021-11-04T07:28:21.887488Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_val.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:21.889846Z","iopub.execute_input":"2021-11-04T07:28:21.890120Z","iopub.status.idle":"2021-11-04T07:28:21.902558Z","shell.execute_reply.started":"2021-11-04T07:28:21.890075Z","shell.execute_reply":"2021-11-04T07:28:21.901360Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(20, 10))\naxs[0].imshow(x_train[50])\naxs[1].imshow(np.argmax(y_train[50], axis=2))","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:21.905686Z","iopub.execute_input":"2021-11-04T07:28:21.906165Z","iopub.status.idle":"2021-11-04T07:28:22.462726Z","shell.execute_reply.started":"2021-11-04T07:28:21.906084Z","shell.execute_reply":"2021-11-04T07:28:22.462014Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**NOW, We are ready to create the Model, U-NET**","metadata":{}},{"cell_type":"code","source":"def unet(num_classes = 23, image_shape = (256, 256, 3)):\n    # Input\n    inputs = Input(image_shape)\n    # Encoder Path\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(inputs)\n    conv1 = Conv2D(64, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv1)\n    pool1 = MaxPooling2D((2,2))(conv1)\n    \n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool1)\n    conv2 = Conv2D(128, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv2)\n    pool2 = MaxPooling2D((2,2))(conv2)\n\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool2)\n    conv3 = Conv2D(256, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv3)\n    pool3 = MaxPooling2D((2,2))(conv3)\n    \n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool3)\n    conv4 = Conv2D(512, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D((2,2))(drop4)\n    \n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(pool4)\n    conv5 = Conv2D(1024, 3, activation='relu', kernel_initializer = 'he_normal', padding='same')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n    \n    # Decoder Path\n    up6 = Conv2D(512, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(drop5))\n    merge6 = concatenate([up6, conv4], axis = 3)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge6)\n    conv6 = Conv2D(512, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n    \n    up7 = Conv2D(256, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv6))\n    merge7 = concatenate([up7, conv3], axis = 3)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge7)\n    conv7 = Conv2D(256, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n    \n    up8 = Conv2D(128, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv7))\n    merge8 = concatenate([up8, conv2], axis = 3)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge8)\n    conv8 = Conv2D(128, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n    \n    up9 = Conv2D(64, 2, activation='relu', kernel_initializer='he_normal', padding='same')(UpSampling2D(size=(2,2))(conv8))\n    merge9 = concatenate([up9, conv1], axis = 3)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(merge9)\n    conv9 = Conv2D(64, 3, activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n    \n    conv10 = Conv2D(num_classes, (1, 1), padding='same', activation='softmax')(conv9)\n    \n    model = Model(inputs, conv10)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:22.463936Z","iopub.execute_input":"2021-11-04T07:28:22.464383Z","iopub.status.idle":"2021-11-04T07:28:22.496192Z","shell.execute_reply.started":"2021-11-04T07:28:22.464330Z","shell.execute_reply":"2021-11-04T07:28:22.495340Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model = unet()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:22.497808Z","iopub.execute_input":"2021-11-04T07:28:22.498225Z","iopub.status.idle":"2021-11-04T07:28:22.722854Z","shell.execute_reply.started":"2021-11-04T07:28:22.498036Z","shell.execute_reply":"2021-11-04T07:28:22.721688Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"plot_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:22.727495Z","iopub.execute_input":"2021-11-04T07:28:22.727715Z","iopub.status.idle":"2021-11-04T07:28:24.277539Z","shell.execute_reply.started":"2021-11-04T07:28:22.727672Z","shell.execute_reply":"2021-11-04T07:28:24.276792Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Reaching that point successfully, we are now ready to complie and train our model","metadata":{}},{"cell_type":"markdown","source":"Although accuracy is not the better metric while doing semantic segmentation, we will just go through this notebook.","metadata":{}},{"cell_type":"code","source":"model_checkpoint = ModelCheckpoint('unet_model.hdf5', monitor='val_loss', verbose=1, save_best_only=True)\nmodel_earlyStopping = EarlyStopping(min_delta= 0.001, patience=30)\n\nmodel.compile(optimizer='adam', loss=['categorical_crossentropy'], metrics=['accuracy'])\n\nhistory = model.fit(x=x_train, y=y_train,\n              validation_data=(x_val, y_val),\n              batch_size=16, epochs=200,\n              callbacks=[model_checkpoint, model_earlyStopping])","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:28:24.279337Z","iopub.execute_input":"2021-11-04T07:28:24.279880Z","iopub.status.idle":"2021-11-04T07:54:25.063241Z","shell.execute_reply.started":"2021-11-04T07:28:24.279826Z","shell.execute_reply":"2021-11-04T07:54:25.062207Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:54:25.068655Z","iopub.execute_input":"2021-11-04T07:54:25.069111Z","iopub.status.idle":"2021-11-04T07:54:25.607522Z","shell.execute_reply.started":"2021-11-04T07:54:25.068930Z","shell.execute_reply":"2021-11-04T07:54:25.606705Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Now, Let's evaluate our Model. I will use the test Image that the model had never seen before.","metadata":{}},{"cell_type":"code","source":"m1 = test_image1\npred = model.predict(np.expand_dims(m1, 0))\npred_mask = np.argmax(pred, axis=-1)\nprint(pred_mask.shape)\npred_mask = pred_mask[0]\nprint(pred_mask.shape)\n\nm2 = test_image2\npred2 = model.predict(np.expand_dims(m2, 0))\npred_mask2 = np.argmax(pred2, axis=-1)\nprint(pred_mask2.shape)\npred_mask2 = pred_mask2[0]\nprint(pred_mask2.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:54:25.613248Z","iopub.execute_input":"2021-11-04T07:54:25.616546Z","iopub.status.idle":"2021-11-04T07:54:26.252664Z","shell.execute_reply.started":"2021-11-04T07:54:25.616482Z","shell.execute_reply":"2021-11-04T07:54:26.250231Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(20, 10))\naxs[0].imshow(m1)\naxs[0].set_title('Image')\naxs[1].imshow(np.argmax(test_label1, axis=-1))\naxs[1].set_title('Ground Truth')\naxs[2].imshow(pred_mask)\naxs[2].set_title('Prediction')","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:56:05.282552Z","iopub.execute_input":"2021-11-04T07:56:05.282868Z","iopub.status.idle":"2021-11-04T07:56:05.949525Z","shell.execute_reply.started":"2021-11-04T07:56:05.282815Z","shell.execute_reply":"2021-11-04T07:56:05.948642Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(20, 10))\naxs[0].imshow(m2)\naxs[0].set_title('Image')\naxs[1].imshow(np.argmax(test_label2, axis=-1))\naxs[1].set_title('Ground Truth')\naxs[2].imshow(pred_mask2)\naxs[2].set_title('Prediction')","metadata":{"execution":{"iopub.status.busy":"2021-11-04T07:56:13.954669Z","iopub.execute_input":"2021-11-04T07:56:13.955003Z","iopub.status.idle":"2021-11-04T07:56:14.585473Z","shell.execute_reply.started":"2021-11-04T07:56:13.954951Z","shell.execute_reply":"2021-11-04T07:56:14.584625Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}